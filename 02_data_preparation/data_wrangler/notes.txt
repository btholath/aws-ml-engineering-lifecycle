# Optional:
02_data_preparation/data_wrangler/
â”œâ”€â”€ 01_generate_flow.py        # Create a .flow JSON programmatically
â”œâ”€â”€ 02_upload_flow.py          # Upload the .flow file to S3
â”œâ”€â”€ template.flow              # Optional template or version-controlled flow
â””â”€â”€ __init__.py

This stage lets you:
Automate creation of a .flow file (JSON) that describes your transformations.
Upload that .flow to S3 so you can open it in SageMaker Studio.
From Studio, you can visually apply transforms or export a processed dataset.


chmod +x 02_data_preparation/data_wrangler/run_data_wrangler_flow.sh
./02_data_preparation/data_wrangler/run_data_wrangler_flow.sh

- check if the file is created ?
aws s3 ls s3://btholath-sagemaker-datawrangler-demo/data-wrangler/flows/customer_sales_cleaning.flow

- Then:
ğŸ” Go to SageMaker Studio â†’ Data Wrangler â†’ Import Flow â†’ From S3
